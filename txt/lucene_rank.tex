\section{Lucene}


Lucene jest biblioteką napisaną w Javie. Framework ten jest w stanie indeksować dużą ilość dokumentów z różnych źródeł i przeprowadzać szybkie wyszukiwania w tych tekstach. W opisywanej aplikacji framework Lucene przechowuje również źródła stron. 

Odnośniki do tych strony zostały pobrane z  serwisu delicous. Następnie dla wszystkich odnośników pobierana jest treść strony. Każda z tych pobranych stron jest następnie oczyszczana ze znaczników HTML i przekazana do zapisania na dysk przy użyciu Lucene.

Do Lucene zapisywane są następujące informacje: identyfikator $id$ dokumentu z bazy danych, oraz przetworzony tekst strony WWW. Przechowywanie identyfikatora dokumentu w danych Lucene pozwala na  późniejsze powiązanie wyników wyszukiwania z odpowiednim rekordem w bazie danych. 

W czasie zapisywania Lucene przeprowadza dodatkowe operacje na tekstach stron. Czynności te pozwolą na lepsze i szybsze indeksowanie i wyszukiwanie. Są to miedzy innymi tokenizacja, usunięcie końcówek wyrazów, usunięcie tzw 'stop-words'. 

%Lucene łączy wyszukiwanie w przestrzeni wektorowej z wyszukiwaniem binarnym.

%http://lucene.apache.org/core/old_versioned_docs/versions/3_0_0/api/core/org/apache/lucene/analysis/package-summary.html \cite{lucene_analysis:online}.



Tokenizacja polega na rozbiciu tekstu na tokeny reprezentujące pojedyncze, indeksowane słowa. Ma to duży wpływ na późniejsze wyszukiwanie w tekscie. Po tokenizacji następuje proces mający na celu otrzymanie trzonu (ang. stem) danego słowa. Powodem tej czynności jest to, że w tekście występują różne gramatyczne wariacje słów. Dzięki temu, słowo 'rowery' zostanie  zamienione na wyraz 'rower'. Jeśli będziemy wyszukiwać dokumenty zawierające słowo 'rower', otrzymamy również te, zawierające różne formy tego słowa, np: 'rowery' czy 'rowerowy'. 

Następnie z tekstu zostają usunięte stop-słowa (ang. stop-words). Są to wyrazy, nie wnoszące dużo do treści tekstu. W języku angielskim będą to na przykład słowa: 'the', 'a', 'and', 'about', 'what', ... Usunięcie tych słów przyśpiesza proces wyszukiwania i przyczynia się do poprawy jakości wyników.  Kolejną czynnością jest normalizacja tekstu w trakcie której następuje usunięcie akcentów ze słów. 


Ponieważ znaczna część posiadanych w bazie danych tekstów jest w języku angielskim, używana jest tylko lista stop-słów z języka angielskiego. Podobnie przy wyszukiwani trzonów słowa i normalizacji używana jest gramatyka języka angielskiego

\subsection*{Wyszukiwanie}

Lucene łączy w sobie model binarny z modelem przestrzeni wektorowej (ang. Vector Space Model). Po otrzymaniu zapytania, na początku przeprowadzane jest wyszukiwanie w modelu binarnym. Wyszukiwanie przy użyciu modelu binarnego wskazuje dokumenty zawierające dane słowa. Możliwe jest użycie dzięki temu wyrażeń logicznych: 'AND', 'OR',... w zapytaniu. Otrzymujemy dzięki temu zdecydowanie mniejszy zbiór dokumentów. Następnie na tych dokumentach następuje wyszukiwanie przy pomocy modelu wektorowego. 

W tym modelu, dokumenty reprezentowane są jako wektory ważone w wielowymiarowej przestrzeni. Każdy element wektora odpowiada termom z dokumentu, a jego wartość wyliczana jest przy użyciu funkcji tf-idf. Tf-idf informuje o częstości wystąpienia termów uwzględniając jednocześnie odpowiednie wyważenie znaczenia lokalnego termu i jego znaczenia w kontekście pełnej kolekcji dokumentów \cite{wikipedia:tf_idf}.

\begin{equation}
(tf-idf)_{i,j} = tf_{i, j} \times idf_i
\end{equation}

$tf_{i, j}$ to częstotliwość występowania  danego termu w dokumencie (ang. term frequency), wyraża się wzorem:
\begin{equation}
tf_{i, j} = \frac{n_{i,j}}{\sum_k n_{k,j}}
\end{equation}
Gdzie $n_{i,j}$ jest liczbą wystąpień termu ($t_{i}$) w dokumencie $d_{j}$, a mianownik jest sumą liczby wystąpień wszystkich termów w dokumencie $d_{j}$.

$idf_i$ to odwrotna częstość w dokumentach (ang. inverse document frequency), wyrażana wzorem:
\begin{equation}
idf_i= \log \frac{|D|}{|\{d \in D : t_i \in d\}|}
\end{equation}


Gdzie:
\begin{itemize}
\item $|D|$ liczba wszystkich dokumentów,
\item $|\{d : t_{i} \in d\}|$  : liczba dokumentów zawierających przynajmniej jedno wystąpienie danego termu
\end{itemize}


Zapytanie jest przekazywane do frameworku, w którym jest one przekształcane na termy. Dla zapytania $q$ i dla każdego dokumentu $d$ wyliczana jest wartość funkcji $score(q,d)$:

\begin{equation}
\begin{split}
score(query,doc) = coord(query, doc) \cdot queryNorm(query) \cdot  \\
\sum_{t \in q} { \big( tf (t \in q) \cdot idf(t)^2 \big)}
\end{split}
\end{equation}


Gdzie:
\begin{itemize}
\item tf, idf - Częstość termów i odwrotna częstość w dokumentach, które zostały opisane powyżej.
\item $coord (query, doc)$ - wspolczynnik wyliczany na powstanie
ilosci termów zapytania, które znajdują sie w wybranym dokumencie.
Najczęscie, dokument w którym znajduje sie więcej termów zapytania
otrzyma wiekszy wynik niz dokument z mniejsza ilością.
\item $queryNorm(query)$ - czynnik normalizujacy wynik. Jego zadanie
polega na umożliwieniu porównywania wyników różnych zapytań.
\end{itemize}

Czas wyszukiwania zapytania w dokumentach przechowywanych Lucene jest szybkie. Przy małej ilości danych (poniżej 1GB danych), wyszukiwanie następuje praktycznie w czasie rzeczywistym. Wynikiem są dokumenty posortowane według wyniku funkcji $score$ \cite{lucene_scoring:online}.


